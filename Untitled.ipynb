{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dgl.nn.pytorch import GATConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Encoder\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}=\\mathrm{MLP}(o_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObsEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, o_dim=128, h_dim=128):\n",
    "        super(ObsEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim. h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, o_dim)\n",
    "    \n",
    "    def forward(self, o):\n",
    "        o = F.relu(self.fc1(o))\n",
    "        o = F.relu(self.fc2(o))\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Kernel\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i,j}^{m}=\\frac{\\exp(\\tau\\cdot \\mathbf{W}_{Q}^{m}h_{i}\\cdot(\\mathbf{W}_{K}^{m}h_{j})^\\top)}{\\sum_{k\\in\\mathbb{B}_{+i}}\\exp(\\tau\\cdot\\mathbf{W}_{Q}^{m}h_{i}\\cdot(\\mathbf{W}_{K}^{m}h_{k})^{\\top})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}^{'}=\\sigma\\left( \\mathrm{concat}_{m\\in M}\\left[ \\sum_{j\\in\\mathbb{B}_{+i}}\\alpha_{i,j}^{m}\\mathbf{W}_{v}^{m}h_{j} \\right] \\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(DotGATLayer, self).__init__()\n",
    "        self.fc_q = nn.Linear(in_dim, out_dim)\n",
    "        self.fc_k = nn.Linear(in_dim, out_dim)\n",
    "        self.fc_v = nn.Linear(in_dim, out_dim)\n",
    "        self.tau = 1/math.sqrt(h_dim)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        k = self.fc_k(edges.src['z'])\n",
    "        q = self.fc_q(edges.dst['z'])\n",
    "        a = torch.matmul(q, k.transpose(-2, -1))*self.tau\n",
    "        return {'e': a}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "    \n",
    "    def reduce_func(self, nodes):\n",
    "        s = nodes.mailbox['e']\n",
    "        alpha = F.softmax(s, dim=1)\n",
    "        v = self.fc_v(nodes.mailbox['z'])\n",
    "        h = torch.sum(alpha * v, dim=1)\n",
    "        return {'h': h, 'alpha': alpha}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        g.ndata['z'] = z\n",
    "        g.apply_edges(self.edge_attention)\n",
    "        g.update_all(self.message_func, self.reduce_func)\n",
    "        h = g.ndata.pop('h')\n",
    "        alpha = g.ndata.pop('alpha')\n",
    "        return h, alpha\n",
    "\n",
    "class MultiHeadDotGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim,\n",
    "                 num_heads, merge='cat'):\n",
    "        super(MultiHeadDotGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        h_dim = out_dim // num_heads\n",
    "        assert (h_dim*num_heads) == out_dim\n",
    "        for _ in range(num_heads):\n",
    "            self.heads.append(DotGATLayer(in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        \n",
    "        hs, alphas = map(list, zip(*[head(g, h)\n",
    "                                     for head in self.heads]))\n",
    "        if self.merge == 'cat':\n",
    "            h = torch.cat(head_outs, dim=1)\n",
    "            alpha = torch.cat(alphas, dim=1)\n",
    "            return h, alpha\n",
    "        else:\n",
    "            return torch.mean(torch.stack(head_outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGN Agent\n",
    "\n",
    "\\begin{equation}\n",
    "Q(o_{i}, \\cdot)=\\mathrm{Linear}\\left(\\mathrm{concat}\\left[ h_{i}, h_{i}^{'}, h_{i}^{''} \\right]\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGNAgent(nn.Module):\n",
    "    def __init__(self, in_dim, act_dim,\n",
    "                 h_dim=128, num_heads=8):\n",
    "        super(DGNAgent, self).__init__()\n",
    "        self.encoder = ObsEncoder(in_dim, h_dim)\n",
    "        self.conv1 = MultiHeadDotGATLayer(h_dim, h_dim, num_heads)\n",
    "        self.conv2 = MultiHeadDotGATLayer(h_dim, h_dim, num_heads)\n",
    "        self.fc_out = nn.Linear(3*h_dim, act_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dummy():\n",
    "    return 1, 2\n",
    "\n",
    "x, y = map(list, zip(*[dummy() for i in range(10)]))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
