{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Encoder\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}=\\mathrm{MLP}(o_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObsEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, o_dim=128, h_dim=512):\n",
    "        super(ObsEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim. h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, o_dim)\n",
    "    \n",
    "    def forward(self, o):\n",
    "        o = F.relu(self.fc1(o))\n",
    "        o = F.relu(self.fc2(o))\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Kernel\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i,j}^{m}=\\frac{\\exp(\\tau\\cdot \\mathbf{W}_{Q}^{m}h_{i}\\cdot(\\mathbf{W}_{K}^{m}h_{j})^\\top)}{\\sum_{k\\in\\mathbb{B}_{+i}}\\exp(\\tau\\cdot\\mathbf{W}_{Q}^{m}h_{i}\\cdot(\\mathbf{W}_{K}^{m}h_{k})^{\\top})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}^{'}=\\sigma\\left( \\mathrm{concat}_{m\\in M}\\left[ \\sum_{j\\in\\mathbb{B}_{+i}}\\alpha_{i,j}^{m}\\mathbf{W}_{v}^{m}h_{j} \\right] \\right)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(DotGATLayer, self).__init__()\n",
    "        self.fc_q = nn.Linear(in_dim, out_dim)\n",
    "        self.fc_k = nn.Linear(in_dim, out_dim)\n",
    "        self.fc_v = nn.Linear(in_dim, out_dim)\n",
    "        self.tau = 1/math.sqrt(out_dim)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        k = self.fc_k(edges.src['z'])\n",
    "        q = self.fc_q(edges.dst['z'])\n",
    "        a = (k*q).sum(-1, keepdims=True)*self.tau\n",
    "        return {'e': a}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "    \n",
    "    def reduce_func(self, nodes):\n",
    "        s = nodes.mailbox['e']\n",
    "        alpha = F.softmax(s, dim=1)\n",
    "        v = self.fc_v(nodes.mailbox['z'])\n",
    "        h = torch.sum(alpha * v, dim=1)\n",
    "        return {'h': h, 'alpha': alpha.squeeze()}\n",
    "\n",
    "    def forward(self, g, z):\n",
    "        g.ndata['z'] = z\n",
    "        g.apply_edges(self.edge_attention)\n",
    "        g.update_all(self.message_func, self.reduce_func)\n",
    "        h = g.ndata.pop('h')\n",
    "        alpha = g.ndata.pop('alpha')\n",
    "        return h, alpha\n",
    "\n",
    "class MultiHeadDotGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim,\n",
    "                 num_heads, merge='cat'):\n",
    "        super(MultiHeadDotGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        h_dim = out_dim // num_heads\n",
    "        assert (h_dim*num_heads) == out_dim\n",
    "        for _ in range(num_heads):\n",
    "            self.heads.append(DotGATLayer(in_dim, h_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        hs, alphas = map(list, zip(*[head(g, h)\n",
    "                                     for head in self.heads]))\n",
    "        alpha = torch.stack(alphas).mean(0)\n",
    "        if self.merge == 'cat':\n",
    "            h = torch.cat(hs, dim=1)\n",
    "            return h, alpha\n",
    "        else:\n",
    "            return torch.mean(torch.stack(hs)), alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGN-R Agent\n",
    "\n",
    "\\begin{equation}\n",
    "Q(o_{i}, \\cdot)=\\mathrm{Linear}\\left(\\mathrm{concat}\\left[ h_{i}, h_{i}^{'}, h_{i}^{''} \\right]\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\mathrm{reg}}(\\theta)=\\frac{1}{M}\\sum_{m=1}^{M}D_{\\mathrm{KL}}\\left( \\mathcal{G}_{m}(O_{i,\\mathcal{C}};\\theta) || \\mathcal{G}_{m}(O_{i,\\mathcal{C}}';\\theta) \\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('graph', 'action', 'reward', 'next_graph', 'done'))\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save Transitions\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        graphs = [sample[0] for sample in samples]\n",
    "        actions = [sample[1] for sample in samples]\n",
    "        rewards = [sample[2] for sample in samples]\n",
    "        next_graphs = [sample[3] for sample in samples]\n",
    "        dones = [sample[4] for sample in samples]\n",
    "\n",
    "        ret_graph = dgl.batch(graphs)\n",
    "        ret_action = torch.stack(actions).reshape(-1, 1)\n",
    "        ret_reward = torch.Tensor(rewards).reshape(-1)\n",
    "        ret_next_graph = dgl.batch(next_graphs)\n",
    "        ret_dones = torch.Tensor(dones).reshape(-1)\n",
    "\n",
    "        \n",
    "        return ret_graph, ret_action, ret_reward, ret_next_graph, ret_dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGN_Conv(nn.Module):\n",
    "    def __init__(self, obs_dim, h_dim=128, num_heads=8,\n",
    "                 target=False):\n",
    "        super(DGN_Q_Net, self).__init__()\n",
    "        self.encoder = ObsEncoder(in_dim=obs_dim, o_dim=h_dim)\n",
    "        self.conv1 = MultiHeadDotGATLayer(h_dim, h_dim, num_heads)\n",
    "        self.conv2 = MultiHeadDotGATLayer(h_dim, h_dim, num_heads)\n",
    "        self.target = target\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        obs = graph.ndata['obs']\n",
    "        z1 = self.encoder(obs)\n",
    "        z2, _ = self.conv1(z1)\n",
    "        z3, alpha = self.conv2(z2)\n",
    "        out = torch.cat([z1, z2, z3], dim=1)\n",
    "        if self.target:\n",
    "            return out\n",
    "        return out, alpha\n",
    "\n",
    "class DGNAgent(nn.Module):\n",
    "    def __init__(self, n_agents, obs_dim, act_dim, h_dim=128,\n",
    "                 num_heads=8, gamma=0.96, batch_size=10,\n",
    "                 buffer_size=2*1e5, epsilon=0.6, episilon_min=0.01,\n",
    "                 decay_rate=0.996, lr=1e-4, neighbors=3, tau=0.25,\n",
    "                 lamb=0.03, beta=0.01, *args, **kwargs):\n",
    "        super(DGNAgent, self).__init__()\n",
    "        self.conv_net = DGN_Conv(obs_dim, h_dim, num_heads)\n",
    "        self.target_conv = DGN_Conv(obs_dim, h_dim, num_heads, target=True)\n",
    "        self.q_net = nn.Linear(3*h_dim, act_dim)\n",
    "        self.target_q = nn.Linear(3*h_dim, act_dim)\n",
    "        self.target_conv.load_state_dict(self.conv_net.state_dict())\n",
    "        self.target_q.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = Adam(self.conv_net.parameters()+self.q_net.parameters(),\n",
    "                              lr=lr)\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        self.n_agents = n_agents\n",
    "        self.n_act = act_dim\n",
    "\n",
    "    def get_action(self, graph):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = torch.randint(0, n_act, size=(self.n_agents,))\n",
    "        else:\n",
    "            q_value = self.q_net(graph)\n",
    "            action = q.argmax(dim=-1).detach()\n",
    "        self.epsilon = max(self.epsilon*self.decay_rate, self.epsilon_min)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_q(self, graph):\n",
    "        z, weight = self.conv_net(graph)\n",
    "        q = self.q_net(z)\n",
    "        return q, weight\n",
    "    \n",
    "    def get_target(self, graph):\n",
    "        z = self.target_conv(graph)\n",
    "        q = self.target_q(z)\n",
    "        return q\n",
    "    \n",
    "    def save_samples(self, g, a, r, n_g, t):\n",
    "        self.buffer.push(g, a, r, n_g, t)\n",
    "    \n",
    "    def fit(self):\n",
    "        if len(self.buffer) < self.batch_size*10:\n",
    "            return False, 0\n",
    "        \n",
    "        state, act, reward, n_state, done = self.buffer.sample(self.batch_size)\n",
    "        curr_qs, curr_weight = self.get_qs(state)\n",
    "        selected_qs = curr_qs.gather(1, act).reshape(-1)\n",
    "        next_qs = self.get_target(n_state).max(dim=1)[0].detach()\n",
    "        target = reward + self.gamma * next_qs * (1 - done)\n",
    "        \n",
    "        _, next_weight = self.get_qs(n_state)\n",
    "        KL = (curr_weight * torch.log(curr_weight/next_weight)).sum(-1)\n",
    "        \n",
    "        loss = F.mse_loss(curr_qs, target) + self.lamb*KL.mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.target_update()\n",
    "        \n",
    "        return True, loss.item()\n",
    "    \n",
    "    def target_update(self):\n",
    "        for target, param in zip(self.target_conv.parameters(),\n",
    "                                  self.conv_net.parameters()):\n",
    "            target.data = (1-self.beta)*target.data + self.beta*param.data\n",
    "\n",
    "        for target, param in zip(self.q_net.parameters(),\n",
    "                                  self.target_q.parameters()):\n",
    "            target.data = (1-self.beta)*target.data + self.beta*param.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bq173\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\dgl\\base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "feature_dim = 3\n",
    "num_nodes = 5\n",
    "\n",
    "def get_fully_connected_edges(num_agents, self_edge=True):\n",
    "    from_idx = []\n",
    "    to_idx = []\n",
    "    for i in range(num_agents):\n",
    "        for j in range(num_agents):\n",
    "            if self_edge:\n",
    "                from_idx.append(i)\n",
    "                to_idx.append(j)\n",
    "            else:\n",
    "                if i!=j:\n",
    "                    from_idx.append(i)\n",
    "                    to_idx.append(j)\n",
    "    return from_idx, to_idx\n",
    "\n",
    "edge_from, edge_to = get_fully_connected_edges(num_nodes)\n",
    "\n",
    "ex_graph = dgl.DGLGraph()\n",
    "ex_graph.add_nodes(num_nodes)\n",
    "ex_graph.add_edges(edge_from, edge_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1323, 0.2330, 0.1995, 0.2321, 0.2031],\n",
      "        [0.1354, 0.2126, 0.2086, 0.2276, 0.2157],\n",
      "        [0.1538, 0.2071, 0.2219, 0.2267, 0.1905],\n",
      "        [0.1535, 0.2047, 0.2191, 0.2241, 0.1986],\n",
      "        [0.2957, 0.1780, 0.2056, 0.1761, 0.1447]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "node_feature = torch.randn(num_nodes, feature_dim)\n",
    "attn = MultiHeadDotGATLayer(feature_dim, feature_dim, num_heads=1)\n",
    "h, alpha = attn(ex_graph, node_feature)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
