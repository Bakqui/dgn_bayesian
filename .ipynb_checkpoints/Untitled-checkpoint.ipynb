{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dgl.nn.pytorch import GATConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Kernel\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i,j}^{m}=\\frac{\\exp(\\tau\\cdot \\mathbf{W}_{Q}^{m}h_{i}\\cdot(\\mathbf{W}_{K}^{m}h_{j})^\\top)}{\\sum_{k\\in\\mathbb{B}_{+i}}\\exp(\\tau\\cdot\\mathbf{W}_{Q}^{m}h_{i}\\cdot(\\mathbf{W}_{K}^{m}h_{k})^{\\top})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim):\n",
    "        super(DotGATLayer, self).__init__()\n",
    "        self.q_fc = nn.Linear(in_dim, h_dim)\n",
    "        self.k_fc = nn.Linear(in_dim, h_dim)\n",
    "        self.v_fc = nn.Linear(in_dim, h_dim)\n",
    "        \n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        \n",
    "        self.sigma = torch.tensor(sigma).type(torch.float32)\n",
    "        self.se_fc1 = nn.Linear(in_dim, se_dim)\n",
    "        self.se_fc2 = nn.Linear(se_dim, 1)\n",
    "        self.se_act = nn.ReLU()\n",
    "        self.sigma_0 = torch.tensor(sigma_0).type(torch.float32)\n",
    "        self.KL_backward = 0.\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.leaky_relu(self.attn_fc(z2))\n",
    "        k = self.se_fc1(edges.src['k'])\n",
    "        k = self.se_fc2(self.se_act(k))\n",
    "        return {'e': a, 'p': k}\n",
    "    \n",
    "    def message_func(self, edges):\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e'], 'p': edges.data['p']}\n",
    "    \n",
    "    def reduce_func(self, nodes):\n",
    "        s = nodes.mailbox['e']\n",
    "        p = F.softmax(nodes.mailbox['p'], dim=1)\n",
    "        mean_prior = torch.log(p+1e-20)\n",
    "        alpha = F.softmax(s, dim=1)\n",
    "        logprobs = torch.log(alpha+1e-20)\n",
    "        if self.training:\n",
    "            mean_posterior = logprobs - self.sigma**2 / 2\n",
    "            out_weight = F.softmax(mean_posterior + self.sigma*torch.randn_like(logprobs), dim=-1)\n",
    "            KL = torch.log(self.sigma_0 / self.sigma + 1e-20) + (\n",
    "                    self.sigma**2 + (mean_posterior - mean_prior)**2) / (2 * self.sigma_0**2) - 0.5\n",
    "        else:\n",
    "            out_weight = alpha\n",
    "            KL = torch.zeros_like(out_weight)\n",
    "        out_weight = self.attn_drop(out_weight)\n",
    "        h = torch.sum(out_weight * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h, 'kl': KL.mean(dim=1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        g.ndata['k'] = h\n",
    "        z = self.fc(self.feat_drop(h))\n",
    "        g.ndata['z'] = z\n",
    "        g.apply_edges(self.edge_attention)\n",
    "        g.update_all(self.message_func, self.reduce_func)\n",
    "        self.KL_backward = g.ndata['kl'].mean()\n",
    "        return g.ndata.pop('h')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
