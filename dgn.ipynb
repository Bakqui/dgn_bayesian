{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGN with Bayesian Attention Modules (BAM)\n",
    "DGN: https://arxiv.org/abs/1810.09202\n",
    "\n",
    "BAM: https://arxiv.org/abs/2010.10604\n",
    "\n",
    "Envionment: https://arxiv.org/abs/1712.00600\n",
    "\n",
    "Reference codes: https://github.com/mlii/mfrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer for Graph Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('graph', 'action', 'reward', 'next_graph', 'done'))\n",
    "\n",
    "\n",
    "class GraphBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save Transitions\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        graphs = [sample[0] for sample in samples]\n",
    "        actions = [torch.LongTensor(sample[1]) for sample in samples]\n",
    "        rewards = [torch.Tensor(sample[2]) for sample in samples]\n",
    "        next_graphs = [sample[3] for sample in samples]\n",
    "        dones = [torch.Tensor(sample[4]) for sample in samples]\n",
    "\n",
    "        ret_graph = dgl.batch(graphs)\n",
    "        ret_action = torch.cat(actions).reshape(-1, 1)\n",
    "        ret_reward = torch.cat(rewards).reshape(-1)\n",
    "        ret_next_graph = dgl.batch(next_graphs)\n",
    "        ret_dones = torch.cat(dones).reshape(-1)\n",
    "        \n",
    "        return ret_graph, ret_action, ret_reward, ret_next_graph, ret_dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Encoder\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}=\\mathrm{MLP}(o_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObsEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, o_dim=128, h_dim=512):\n",
    "        super(ObsEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, o_dim)\n",
    "    \n",
    "    def forward(self, o):\n",
    "        o = F.relu(self.fc1(o))\n",
    "        o = F.relu(self.fc2(o))\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Kernel\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i,j}^{m}=\\frac{\\exp(\\tau\\cdot \\mathbf{W}_{Q}^{m}h_{i}\\cdot(\\mathbf{W}_{K}^{m}h_{j})^\\top)}{\\sum_{k\\in\\mathbb{B}_{+i}}\\exp(\\tau\\cdot\\mathbf{W}_{Q}^{m}h_{i}\\cdot(\\mathbf{W}_{K}^{m}h_{k})^{\\top})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}^{'}=\\sigma\\left( \\mathrm{concat}_{m\\in M}\\left[ \\sum_{j\\in\\mathbb{B}_{+i}}\\alpha_{i,j}^{m}\\mathbf{W}_{v}^{m}h_{j} \\right] \\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(DotGATLayer, self).__init__()\n",
    "        self.fc_q = nn.Linear(in_dim, out_dim)\n",
    "        self.fc_k = nn.Linear(in_dim, out_dim)\n",
    "        self.fc_v = nn.Linear(in_dim, out_dim)\n",
    "        self.tau = 1/math.sqrt(out_dim)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        k = self.fc_k(edges.src['z'])\n",
    "        q = self.fc_q(edges.dst['z'])\n",
    "        a = (k*q).sum(-1, keepdims=True)*self.tau\n",
    "        return {'e': a}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "    \n",
    "    def reduce_func(self, nodes):\n",
    "        s = nodes.mailbox['e']\n",
    "        alpha = F.softmax(s, dim=1)\n",
    "        v = self.fc_v(nodes.mailbox['z'])\n",
    "        h = torch.sum(alpha * v, dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, g, z):\n",
    "        g.ndata['z'] = z\n",
    "        g.apply_edges(self.edge_attention)\n",
    "        g.update_all(self.message_func, self.reduce_func)\n",
    "        h = g.ndata.pop('h')\n",
    "        # delete all unnecessary variables\n",
    "        g.ndata.pop('z')\n",
    "        g.edata.pop('e')\n",
    "        return h\n",
    "\n",
    "class MultiHeadDotGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads):\n",
    "        super(MultiHeadDotGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        h_dim = out_dim // num_heads\n",
    "        assert (h_dim*num_heads) == out_dim\n",
    "        for _ in range(num_heads):\n",
    "            self.heads.append(DotGATLayer(in_dim, h_dim))\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        hs = [head(g, h) for head in self.heads]\n",
    "        h = F.relu(torch.cat(hs, dim=1))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Kernel with Bayesian Attention\n",
    "\n",
    "### Latent random variable: unnormalized attention score\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{\\Phi}_{i,j}^{m}=\\tau\\cdot \\mathbf{W}_{Q}^{m}h_{i}\\cdot(\\mathbf{W}_{K}^{m}h_{j})^\\top\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "S_{i,j}^{m}\\sim\\log\\mathcal{N}(\\Phi_{i,j}^{m}-\\sigma^{2}/2,\\sigma^{2})\n",
    "\\end{equation}\n",
    "\n",
    "### Reparametrization\n",
    "\n",
    "\\begin{equation}\n",
    "S_{i,j}^{m}=g_{\\phi}(\\epsilon_{i,j}^{m})=\\exp(\\mathrm{\\Phi}_{i,j}^{m})\\exp(\\epsilon_{i,j}^{m}\\sigma-\\sigma^{2}/2)\n",
    "\\\\\\epsilon_{i,j}^{m}\\sim\\mathcal{N}(0,1)\n",
    "\\end{equation}\n",
    "\n",
    "### Key-based contextual prior\n",
    "\n",
    "\\begin{equation}\n",
    "\\Psi_{i,j}^{m}=\\text{softmax}\\left(\\mathrm{MLP}(h_{j})\\right)\n",
    "\\\\\n",
    "p_{\\eta}(S_{i,j}^{m})=\\log\\mathcal{N}(\\Psi_{i,j}^{m},\\sigma_{0}^{2})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### KL divergence\n",
    "\n",
    "\\begin{equation}\n",
    "D_{\\mathrm{KL}}\\left(q_{\\phi}(S_{i,j}^{m})||p_{\\eta}(S_{i,j}^{m})\\right)=\\log\\frac{\\sigma_{0}}{\\sigma}+\\frac{\\sigma^{2}+\\left(\\Phi_{i,j}^{m}-\\Psi_{i,j}^{m}-\\sigma^{2}/2\\right)^{2}}{2\\sigma_{0}^{2}}\n",
    "\\end{equation}\n",
    "\n",
    "### Bayesian attention\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{i,j}^{m}=\\frac{S_{i,j}^{m}}{\\sum_{k\\in\\mathbb{B}_{+i}}S_{i,k}^{m}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}^{'}=\\sigma\\left( \\mathrm{concat}_{m\\in M}\\left[ \\sum_{j\\in\\mathbb{B}_{+i}}\\alpha_{i,j}^{m}\\mathbf{W}_{v}^{m}h_{j} \\right] \\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim,\n",
    "                 se_dim, sigma, sigma_0):\n",
    "        super(BayesGATLayer, self).__init__()\n",
    "        self.fc_q = nn.Linear(in_dim, out_dim)\n",
    "        self.fc_k = nn.Linear(in_dim, out_dim)\n",
    "        self.fc_v = nn.Linear(in_dim, out_dim)\n",
    "        self.tau = 1/math.sqrt(out_dim)\n",
    "\n",
    "        self.sigma = torch.tensor(sigma).type(torch.float32)\n",
    "        self.se_fc1 = nn.Linear(in_dim, se_dim)\n",
    "        self.se_fc2 = nn.Linear(se_dim, 1)\n",
    "        self.se_act = nn.ReLU()\n",
    "        self.sigma_0 = torch.tensor(sigma_0).type(torch.float32)\n",
    "        self.KL_backward = 0.\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        k = edges.src['z']\n",
    "        k2 = self.se_fc1(k)\n",
    "        k2 = self.se_fc2(self.se_act(k2))\n",
    "        k = self.fc_k(k)\n",
    "        q = self.fc_q(edges.dst['z'])\n",
    "        a = (k*q).sum(-1, keepdims=True)*self.tau\n",
    "        \n",
    "        return {'e': a, 'p': k2}\n",
    "    \n",
    "    def message_func(self, edges):\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e'], 'p': edges.data['p']}\n",
    "    \n",
    "    def reduce_func(self, nodes):\n",
    "        s = nodes.mailbox['e']\n",
    "        p = F.softmax(nodes.mailbox['p'], dim=1)\n",
    "        mean_prior = torch.log(p+1e-20)\n",
    "        alpha = F.softmax(s, dim=1)\n",
    "        logprobs = torch.log(alpha+1e-20)\n",
    "        if self.training:\n",
    "            mean_posterior = logprobs - self.sigma**2 / 2\n",
    "            out_weight = F.softmax(mean_posterior + self.sigma*torch.randn_like(logprobs), dim=1)\n",
    "            KL = torch.log(self.sigma_0 / self.sigma + 1e-20) + (\n",
    "                    self.sigma**2 + (mean_posterior - mean_prior)**2) / (2 * self.sigma_0**2) - 0.5\n",
    "        else:\n",
    "            out_weight = alpha\n",
    "            KL = torch.zeros_like(out_weight)\n",
    "        v = self.fc_v(nodes.mailbox['z'])\n",
    "        h = torch.sum(out_weight * v, dim=1)\n",
    "        return {'h': h, 'kl': KL.mean(dim=1)}\n",
    "\n",
    "    def forward(self, g, z):\n",
    "        g.ndata['z'] = z\n",
    "        g.apply_edges(self.edge_attention)\n",
    "        g.update_all(self.message_func, self.reduce_func)\n",
    "        self.KL_backward = g.ndata.pop('kl').mean()\n",
    "        h = g.ndata.pop('h')\n",
    "        # delete all unnecessary variables\n",
    "        g.ndata.pop('z')\n",
    "        g.edata.pop('e')\n",
    "        g.edata.pop('p')\n",
    "        return h\n",
    "\n",
    "class BayesMultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads,\n",
    "                 se_dim, sigma, sigma_0):\n",
    "        super(BayesMultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        h_dim = out_dim // num_heads\n",
    "        assert (h_dim*num_heads) == out_dim\n",
    "        for _ in range(num_heads):\n",
    "            self.heads.append(BayesGATLayer(in_dim, h_dim,\n",
    "                                            se_dim, sigma, sigma_0))\n",
    "        self.KL_backward = 0.\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        hs = [head(g, h) for head in self.heads]\n",
    "        KL = [head.KL_backward for head in self.heads]\n",
    "        self.KL_backward = torch.mean(torch.stack(KL))\n",
    "        h = F.relu(torch.cat(hs, dim=1))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGN Agent\n",
    "\n",
    "\\begin{equation}\n",
    "Q(o_{i}, \\cdot)=\\mathrm{Linear}\\left(\\mathrm{concat}\\left[ h_{i}, h_{i}^{'}, h_{i}^{''} \\right]\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGN_Conv(nn.Module):\n",
    "    def __init__(self, obs_dim, h_dim=128, num_heads=8,\n",
    "                 target=False):\n",
    "        super(DGN_Conv, self).__init__()\n",
    "        self.encoder = ObsEncoder(in_dim=obs_dim, o_dim=h_dim)\n",
    "        self.conv1 = MultiHeadDotGATLayer(h_dim, h_dim, num_heads)\n",
    "        self.conv2 = MultiHeadDotGATLayer(h_dim, h_dim, num_heads)\n",
    "        self.target = target\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        obs = graph.ndata['obs']\n",
    "        z1 = self.encoder(obs)\n",
    "        z2 = self.conv1(graph, z1)\n",
    "        z3 = self.conv2(graph, z2)\n",
    "        out = torch.cat([z1, z2, z3], dim=1)\n",
    "        return out\n",
    "\n",
    "class DGNAgent(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, h_dim,\n",
    "                 num_heads, gamma, batch_size,\n",
    "                 buffer_size, lr, neighbors,\n",
    "                 beta, *args, **kwargs):\n",
    "        super(DGNAgent, self).__init__()\n",
    "        self.conv_net = DGN_Conv(obs_dim, h_dim, num_heads)\n",
    "        self.target_conv = DGN_Conv(obs_dim, h_dim, num_heads, target=True)\n",
    "        self.q_net = nn.Linear(3*h_dim, act_dim)\n",
    "        self.target_q = nn.Linear(3*h_dim, act_dim)\n",
    "        self.target_conv.load_state_dict(self.conv_net.state_dict())\n",
    "        self.target_q.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = Adam(list(self.conv_net.parameters())+list(self.q_net.parameters()),\n",
    "                              lr=lr)\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.buffer = GraphBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.n_act = act_dim\n",
    "        self.n_neighbor = neighbors\n",
    "\n",
    "        self._new_add = 0\n",
    "\n",
    "    def get_action(self, graph, epsilon):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = torch.randint(0, self.n_act, size=(graph.num_nodes(),))\n",
    "        else:\n",
    "            q_value = self.q_net(graph)\n",
    "            action = q_value.argmax(dim=-1).detach()\n",
    "        self.epsilon = max(self.epsilon*self.decay_rate, self.epsilon_min)\n",
    "\n",
    "        return action.numpy().astype(np.int32)\n",
    "\n",
    "    def get_q(self, graph):\n",
    "        z = self.conv_net(graph)\n",
    "        q = self.q_net(z)\n",
    "        return q\n",
    "\n",
    "    def get_target(self, graph):\n",
    "        z = self.target_conv(graph)\n",
    "        q = self.target_q(z)\n",
    "        return q\n",
    "    \n",
    "    def save_samples(self, g, a, r, n_g, t):\n",
    "        self.buffer.push(g, a, r, n_g, t)\n",
    "        self._new_add += 1\n",
    "\n",
    "    def train(self):\n",
    "        batch_num = self._new_add * 2 // self.batch_size\n",
    "        for _ in range(batch_num):\n",
    "            state, act, reward, n_state, done = self.buffer.sample(self.batch_size)\n",
    "            curr_qs = self.get_q(state)\n",
    "            selected_qs = curr_qs.gather(1, act).reshape(-1)\n",
    "            next_qs = self.get_target(n_state).max(dim=1)[0].detach()\n",
    "            target = reward + self.gamma * next_qs * (1 - done)\n",
    "\n",
    "            loss = F.mse_loss(selected_qs, target)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.update()\n",
    "        \n",
    "        self._new_add = 0\n",
    "\n",
    "    def update(self):\n",
    "        for target, param in zip(self.target_conv.parameters(),\n",
    "                                  self.conv_net.parameters()):\n",
    "            target.data = (1-self.beta)*target.data + self.beta*param.data\n",
    "\n",
    "        for target, param in zip(self.target_q.parameters(),\n",
    "                                  self.q_net.parameters()):\n",
    "            target.data = (1-self.beta)*target.data + self.beta*param.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGN Agent with Bayesian Attention\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\mathrm{reg}}(\\phi)=\n",
    "\\lambda_{t}\\sum_{l=1}^{L}D_{\\mathrm{KL}}\\left(q_{\\phi}(S^{l}\\,|\\,g_{\\phi}(\\mathbf{\\epsilon}_{1:l-1}))||p_{\\mathbf{\\eta}}(S^{l}\\,|\\,g_{\\phi}(\\mathbf{\\epsilon}_{1:l-1}))\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesDGN_Conv(nn.Module):\n",
    "    def __init__(self, obs_dim, h_dim, num_heads,\n",
    "                 se_dim, sigma, sigma_0, target=False):\n",
    "        super(BayesDGN_Conv, self).__init__()\n",
    "        self.encoder = ObsEncoder(in_dim=obs_dim, o_dim=h_dim)\n",
    "        self.conv1 = BayesMultiHeadGATLayer(h_dim, h_dim, num_heads,\n",
    "                                            se_dim, sigma, sigma_0)\n",
    "        self.conv2 = BayesMultiHeadGATLayer(h_dim, h_dim, num_heads,\n",
    "                                            se_dim, sigma, sigma_0)\n",
    "        self.target = target\n",
    "        if self.target:\n",
    "            self.training = False\n",
    "\n",
    "    def forward(self, graph):\n",
    "        obs = graph.ndata['obs']\n",
    "        z1 = self.encoder(obs)\n",
    "        z2 = self.conv1(graph, z1)\n",
    "        z3 = self.conv2(graph, z2)\n",
    "        out = torch.cat([z1, z2, z3], dim=1)\n",
    "        return out\n",
    "\n",
    "    def kl(self):\n",
    "        return self.conv1.KL_backward + self.conv2.KL_backward\n",
    "\n",
    "class BayesDGNAgent(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, h_dim,\n",
    "                 num_heads, gamma, batch_size,\n",
    "                 buffer_size, lr, neighbors,\n",
    "                 beta, rho, se_dim, sigma, sigma_0,\n",
    "                 *args, **kwargs):\n",
    "        super(BayesDGNAgent, self).__init__()\n",
    "        self.conv_net = BayesDGN_Conv(obs_dim, h_dim, num_heads,\n",
    "                                      se_dim, sigma, sigma_0)\n",
    "        self.target_conv = BayesDGN_Conv(obs_dim, h_dim, num_heads,\n",
    "                                         se_dim, sigma, sigma_0,\n",
    "                                         target=True)\n",
    "        self.q_net = nn.Linear(3*h_dim, act_dim)\n",
    "        self.target_q = nn.Linear(3*h_dim, act_dim)\n",
    "        self.target_conv.load_state_dict(self.conv_net.state_dict())\n",
    "        self.target_q.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = Adam(list(self.conv_net.parameters())+list(self.q_net.parameters()),\n",
    "                              lr=lr)\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.buffer = GraphBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.n_act = act_dim\n",
    "        self.n_neighbor = neighbors\n",
    "        self.rho = torch.tensor(rho).type(torch.float32)\n",
    "        self.t = 0\n",
    "\n",
    "        self._new_add = 0\n",
    "\n",
    "    def get_action(self, graph, epsilon):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = torch.randint(0, self.n_act, size=(graph.num_nodes(),))\n",
    "        else:\n",
    "            q_value = self.get_q(graph)\n",
    "            action = q_value.argmax(dim=-1).detach()\n",
    "\n",
    "        return action.numpy().astype(np.int32)\n",
    "\n",
    "    def get_q(self, graph):\n",
    "        z = self.conv_net(graph)\n",
    "        self.KL_backward = self.conv_net.kl()\n",
    "        q = self.q_net(z)\n",
    "        return q\n",
    "\n",
    "    def get_target(self, graph):\n",
    "        z = self.target_conv(graph)\n",
    "        q = self.target_q(z)\n",
    "        return q\n",
    "    \n",
    "    def save_samples(self, g, a, r, n_g, t):\n",
    "        self.buffer.push(g, a, r, n_g, t)\n",
    "        self._new_add += 1\n",
    "\n",
    "    def train(self):\n",
    "        batch_num = self._new_add * 2 // self.batch_size\n",
    "        lamb_elbo = F.sigmoid(self.rho*self.t)\n",
    "        for _ in range(batch_num):\n",
    "            state, act, reward, n_state, done = self.buffer.sample(self.batch_size)\n",
    "            curr_qs, curr_weight = self.get_q(state)\n",
    "            selected_qs = curr_qs.gather(1, act).reshape(-1)\n",
    "            next_qs = self.get_target(n_state).max(dim=1)[0].detach()\n",
    "            target = reward + self.gamma * next_qs * (1 - done)\n",
    "\n",
    "            KL = lamb_elbo*self.conv_net.kl()\n",
    "            loss = F.mse_loss(selected_qs, target) + KL\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.update()\n",
    "        self._new_add = 0\n",
    "        self.t += 1\n",
    "\n",
    "    def update(self):\n",
    "        for target, param in zip(self.target_conv.parameters(),\n",
    "                                  self.conv_net.parameters()):\n",
    "            target.data = (1-self.beta)*target.data + self.beta*param.data\n",
    "\n",
    "        for target, param in zip(self.target_q.parameters(),\n",
    "                                  self.q_net.parameters()):\n",
    "            target.data = (1-self.beta)*target.data + self.beta*param.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get graph from observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(feature, n_agents, n_neighbor=3):\n",
    "    from_idx = [] # source\n",
    "    to_idx = [] # destination\n",
    "    dis = []\n",
    "    for src in range(n_agents):\n",
    "        x, y = feature[src][-2], feature[src][-1]\n",
    "        dis.append((x, y, src))\n",
    "    for src in range(n_agents):\n",
    "        f = []\n",
    "        for dst in range(n_agents):\n",
    "            distance = (dis[dst][0]-dis[src][0])**2+(dis[dst][1]-dis[src][1])**2\n",
    "            f.append([distance, dst])\n",
    "        f.sort(key=lambda x:x[0]) # sort w.r.t. distance\n",
    "        for order in range(n_neighbor+1):\n",
    "            from_idx.append(src)\n",
    "            to_idx.append(f[order][1])\n",
    "    return from_idx, to_idx\n",
    "\n",
    "def observation(view, feature, n_agents):\n",
    "    obs = []\n",
    "    for j in range(n_agents):\n",
    "        obs.append(np.hstack(((view[j][:,:,1]-view[j][:,:,5]).flatten(),\n",
    "                              feature[j][-1:-3:-1])))\n",
    "    return obs\n",
    "\n",
    "def gen_graph(view, feature, n_neighbor):\n",
    "    \"\"\"Get state as a graph\"\"\"\n",
    "    g = dgl.DGLGraph()\n",
    "\n",
    "    n_agents = len(feature)\n",
    "    g.add_nodes(n_agents)\n",
    "    \n",
    "    from_idx, to_idx = get_edges(feature, n_agents, n_neighbor)\n",
    "    g.add_edges(from_idx, to_idx)\n",
    "    \n",
    "    # we save observation as the feature of the nodes\n",
    "    obs = observation(view, feature, n_agents)\n",
    "    g.ndata['obs'] = torch.Tensor(obs) # shape = (n_agents, view_size**2 + 2)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-play with \"Battle\" Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, n_round, map_size, max_steps, handles, models, eps,\n",
    "         print_every, n_neighbor=3, render=False, train=True):\n",
    "    \"\"\"play a ground and train\"\"\"\n",
    "    env.reset()\n",
    "    generate_map(env, map_size, handles)\n",
    "\n",
    "    step_ct = 0\n",
    "    done = False\n",
    "\n",
    "    n_group = len(handles)\n",
    "    state = [None for _ in range(n_group)]\n",
    "    next_state = [None for _ in range(n_group)]\n",
    "    acts = [None for _ in range(n_group)]\n",
    "\n",
    "    alives = [None for _ in range(n_group)]\n",
    "    rewards = [None for _ in range(n_group)]\n",
    "    nums = [env.get_num(handle) for handle in handles]\n",
    "    max_nums = nums.copy()\n",
    "\n",
    "    print(\"\\n\\n[*] ROUND #{0}, EPS: {1:.2f} NUMBER: {2}\".format(n_round, eps, nums))\n",
    "    mean_rewards = [[] for _ in range(n_group)]\n",
    "    total_rewards = [[] for _ in range(n_group)]\n",
    "\n",
    "    # get graph from observation of each group\n",
    "\n",
    "    while not done and step_ct < max_steps:\n",
    "        # take actions for every group\n",
    "        for i in range(n_group):\n",
    "            view, feature = env.get_observation(handles[i])\n",
    "            state[i] = gen_graph(view, feature, n_neighbor)\n",
    "            acts[i] = models[i].act(graph=state[i], epsilon=eps)\n",
    "\n",
    "        for i in range(n_group):\n",
    "            env.set_action(handles[i], acts[i])\n",
    "\n",
    "        # simulate one step\n",
    "        done = env.step()\n",
    "\n",
    "        for i in range(n_group):\n",
    "            rewards[i] = env.get_reward(handles[i])\n",
    "            alives[i] = env.get_alive(handles[i])\n",
    "            view, feature = env.get_observation(handles[i])\n",
    "            next_state[i] = gen_graph(view, feature, n_neighbor)\n",
    "\n",
    "        buffer = {\n",
    "            'g': state[0], 'a': acts[0], 'r': rewards[0],\n",
    "            'n_g': next_state[0], 't': ~alives[0]\n",
    "        }\n",
    "\n",
    "        # save experience\n",
    "        if train:\n",
    "            models[0].save_samples(**buffer)\n",
    "\n",
    "        # stat info\n",
    "        nums = [env.get_num(handle) for handle in handles]\n",
    "\n",
    "        for i in range(n_group):\n",
    "            sum_reward = sum(rewards[i])\n",
    "            rewards[i] = sum_reward / nums[i]\n",
    "            mean_rewards[i].append(rewards[i])\n",
    "            total_rewards[i].append(sum_reward)\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # clear dead agents\n",
    "        env.clear_dead()\n",
    "\n",
    "        info = {\"Ave-Reward\": np.round(rewards, decimals=6), \"NUM\": nums}\n",
    "\n",
    "        step_ct += 1\n",
    "\n",
    "        if step_ct % print_every == 0:\n",
    "            print(\"> step #{}, info: {}\".format(step_ct, info))\n",
    "\n",
    "    # train model after an episode ends\n",
    "    if train:\n",
    "        models[0].train()\n",
    "\n",
    "    for i in range(n_group):\n",
    "        mean_rewards[i] = sum(mean_rewards[i]) / len(mean_rewards[i])\n",
    "        total_rewards[i] = sum(total_rewards[i])\n",
    "\n",
    "    return max_nums, nums, mean_rewards, total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play \"Battle\" with trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def battle(env, n_round, map_size, max_steps, handles, models, eps,\n",
    "           print_every, n_neighbor=3, render=False):\n",
    "    \"\"\"play a ground\"\"\"\n",
    "    env.reset()\n",
    "    generate_map(env, map_size, handles)\n",
    "\n",
    "    step_ct = 0\n",
    "    done = False\n",
    "\n",
    "    n_group = len(handles)\n",
    "    state = [None for _ in range(n_group)]\n",
    "    acts = [None for _ in range(n_group)]\n",
    "\n",
    "    alives = [None for _ in range(n_group)]\n",
    "    rewards = [None for _ in range(n_group)]\n",
    "    nums = [env.get_num(handle) for handle in handles]\n",
    "    max_nums = nums.copy()\n",
    "\n",
    "    print(\"\\n\\n[*] ROUND #{0}, EPS: {1:.2f} NUMBER: {2}\".format(n_round, eps, nums))\n",
    "    mean_rewards = [[] for _ in range(n_group)]\n",
    "    total_rewards = [[] for _ in range(n_group)]\n",
    "\n",
    "    while not done and step_ct < max_steps:\n",
    "        # take actions for every model\n",
    "        for i in range(n_group):\n",
    "            view, feature = env.get_observation(handles[i])\n",
    "            state[i] = gen_graph(view, feature, n_neighbor)\n",
    "            acts[i] = models[i].act(graph=state[i], epsilon=eps)\n",
    "\n",
    "        for i in range(n_group):\n",
    "            env.set_action(handles[i], acts[i])\n",
    "\n",
    "        # simulate one step\n",
    "        done = env.step()\n",
    "\n",
    "        for i in range(n_group):\n",
    "            rewards[i] = env.get_reward(handles[i])\n",
    "            alives[i] = env.get_alive(handles[i])\n",
    "\n",
    "        # stat info\n",
    "        nums = [env.get_num(handle) for handle in handles]\n",
    "\n",
    "        for i in range(n_group):\n",
    "            sum_reward = sum(rewards[i])\n",
    "            rewards[i] = sum_reward / nums[i]\n",
    "            mean_rewards[i].append(rewards[i])\n",
    "            total_rewards[i].append(sum_reward)\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # clear dead agents\n",
    "        env.clear_dead()\n",
    "\n",
    "        info = {\"Ave-Reward\": np.round(rewards, decimals=6), \"NUM\": nums}\n",
    "\n",
    "        step_ct += 1\n",
    "\n",
    "        if step_ct % print_every == 0:\n",
    "            print(\"> step #{}, info: {}\".format(step_ct, info))\n",
    "\n",
    "    for i in range(n_group):\n",
    "        mean_rewards[i] = sum(mean_rewards[i]) / len(mean_rewards[i])\n",
    "        total_rewards[i] = sum(total_rewards[i])\n",
    "\n",
    "    return max_nums, nums, mean_rewards, total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to do\n",
    "\n",
    "1) Train DGN-BAM model (\"./train_battle.py\")\n",
    "\n",
    "2) Play with different models (vs IL, MFQ, DGN, ...) (\"./battle.py)\n",
    "\n",
    "3) Discussion on the results & writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
